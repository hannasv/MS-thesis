\chapter{Discussion}
The downloaded size of \acrshort{ecc} is 17Gb, stored in \acrshort{netcdf}-files. 
In comparison to related works by \citeauthor{precip_nowcasting} (\citeyear{precip_nowcasting}) and \citeauthor{SunAirLSTM} (\citeyear{SunAirLSTM}), using datasets of size X and Y respectively.

Future work speed up the \acrshort{ar}-model computations of self implemented modules for computations using  a C or C++ backend.

Tensorflow har pakker for å ha prosessert data i cache. For å unngå at det må prosesseres for hver epoch. 


%\subsubsection{Varying the test sample}
% Prøv å gjøre dette plottet ikke en hel side da er det lettere for latex å plassere det tror jeg.
%Partition into test-datset, remaining data is used for training. 
%\begin{itemize}[noitemsep, topsep=0pt]
%    \item Test 1 : 2014 - 2018
%    \item Test 2 : 2009 - 2013
%    \item Test 3 : 2004 - 2008
%\end{itemize}

\textbf{A reasonable accurate parameterization should provide a good estimate of the target variable, here cloud cover in isolation. Also on a higher level contribute to a improved model performance in the term of uncertainties attributed to a variable or process.}  It is possible to achieve the same score with several  models. For this particular case it could be an idea to test the response of the model to particular weather phenomenas, that reflect a lot of sunight and would be necessary to hit to get a good over all performance. \textbf{Få et eksempel fra trude} 

It is important to know if the model is performing good for the right reasons.

Based on the small set of experiments run in this section, there is 
Datasets publish have different versions. Propose a suggestion for the content on \acrshort{ecc} v2.

To order to perform more complex experiments than shown in this section, more work need to be done behind the scene's to ensure better flow of data into memory. The combination of batch size and sequence length determine the load on memory per weight update date. The are inverse of each other, for short sequences the batch sizes can be larger. This is not the same as stating that this will be the optimal solution. Who knows the optimal solution might be outside the search space available on the used environment. Advances in hardware, algorithms and X drive advances in machine learning, as mentioned in Section \ref{ch:num_methods} \textbf{add more specific section}. The optimal solution may have always been out of reach. 

Unlike the \cite{}

A method for training deeper models on datasets is to reduce the spatial or temporal resolution. This is done in precipitation nowcasting, moving from 300x300 pixels to 100x100. Arguing that they want to remove noise.

\begin{enumerate}
    \item Argumenter for at andre ikke har trent denne modellen på så store dataset. 
    \item Skyer har en temporal opplæsning på en time så det er uaktelt å endre på denne for å trene modellen. 
\end{enumerate}


The statistical properties are slightly different for the different test sets.  

Short sequence length to reduce the computational complexity of experiments. 

No efforts have been made to assess the impact of the artefact on the performance.

One variable was not found to increase the performance.


data sets can be reduced without significant loss of information

A drawback of this study was not to account for correlations between radiomics features and the ROI, but such corrections were
not performed in this .

Land ocean mask study statistics of subgroups.