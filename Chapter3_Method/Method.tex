\setcounter{chapter}{2}
\chapter{Numerical Methods} \label{ch:num_methods}

In this section we introduce the computational methods used for generating the numerical experiments conducted in this thesis, starting with a short introduction to artificial intelligence. We will give a brief introduction to the biological mechanisms the algorithms in this thesis draw inspiration from. This helps to gain insight to possible applications of different structures. 
%Presenting the autoregressive model and convolutional long short-term memory. The performance metrics used to evaluate their performance and finishing of with automatic optimization routines.
% based on bio-inspired mechanisms are introduced
The task of forecasting in time and space requires two types of intelligence. One is computer vision, to understand the spatial relation and use the underlying physical properties. The other is sequential modelling to understand the temporal evolution.

Two approaches will be explored: autoregressive models (AR) and Convolutional Long Short-Term Memory Network (ConvLSTM). The AR model describes a time varying process, depending linearly on it previous values. The ConvLSTM is used to find a non-linear relation that describes phenomena varying in both time and space. %Another method used is the ConvLSTM.
The aim of this study is to determine whether the concatenation of linear models, or the more advanced non-linear model are better at prediction the complex phenomena varying in time and space.
%When used in tandem (all the linear models?), these models can predict complex natural phenomena ++++ .
The popularity of DL can be partly explained by its flexibility. This flexibility allows deep learning to be applied across many domains. The algorithms discussed here are simply a mathematical framework for learning model representations in data. The process of training is repeated until the network reaches an acceptable performance. In other words \textit{the extent to which this potential can be exploited is limited to the effectiveness of the training procedure applied} and also the data its provided. 

\section{Artificial intelligence}
\input{Chapter3_Method/TiKZ/subcategories_AI}
%\input{Chapter3_Methods/TiKZ/subcategories_AI}
In encounters with geoscientists the author often get the question: \textit{What is the difference between machine learning and artificial intelligence?} They are not different fields, but machine learning (ML) is a subfield of AI. In fact, it is worth mentioning that there is a subfield of ML known as deep learning (DL) (see graph in Figure \ref{fig:subcategories_AI}). DL is at the frontier of AI, with many recent advances being made in this subfield. %Deep learning is a subfield of machine learning, making it a subfield of artificial intelligence. %Deep learning provide a improved. 
The origin of the subfields has a historical explanation. Each subfield is linked to significant advances, which will be explained using parallels to the long standing problem of computer chess. 
\input{Chapter3_Method/TiKZ/one_layer_mlp}
Figure \ref{fig:one_layer_mlp} illustrates a simple artificial neural network. Artificial intelligence (AI) in general and Deep Learning (DL) in particular emerged from biological inspired computing. Many of the DL network architectures draw inspiration from the human brain. The architecture of DL, while distinct from biological computing, is named such that concepts in neuroscience and computing can be treated analogously. For example using building blocks such as neurons (nodes, units), weights (connections between neurons), rules of signal propagation, activation (transfer function) and learning algorithms (training algorithms). % \textbf{Raymond: Må skille mellom AI og DL - AI er ikke nødvendigvis basert på en modell av hjernen.}

The circles illustrate nodes (neurons). Nodes belonging to the same layer is shown in one color. Arrows illustrates weights, the connections between the layers. Nodes belonging to the same layer are not connected, but nodes in consecutive layers are connected with weights. 
% \textbf{cite \href{https://www.sciencedirect.com/topics/engineering/neural-network-architecture}{\textbf{https://www.sciencedirect.com/topics/engineering/neural-network-architecture}}}

% For mye om receptive field.
Sequence modelling draws parallels to the human memory. This type of modelling requires information about earlier stages, this is stored in memory. 
%Image recognition and sequence modelling draw parallels to human vision and memory, respectively. 
Simple models have one memory centre. Drawing inspiration from the brain, other more complex models make the distinction between a short term and a long memory centre. Finishing sentences for others is a trivial task for humans. The reader should not be surprised that by the following \textit{the clouds are in the \ldots sky} \cite{colah_blog_post}.


Three factors determine advances in the field of AI: data, hardware, and algorithms \cite{chollet_book}. This explains why there often is a significant time gap between an idea and breakthroughs in the architectures and results. CNNs, for example, were conceptually developed in the 80s, but a lack of sufficient computing power (hardware) kept their use in hibernation until 2012, when a CNN (AlexNet) won the ImageNet challenge, an image recognition contest \textbf{(Citation)}.

AI started by automating tasks normally performed by humans. Computer chess is the longest studied problem in the history of artificial intelligence and advancements in computer chess provide good examples on the evolution of AI. In 1951, Alan Turing was the first to publish a program, on paper, capable of playing a entire game of chess. 
% In media (and other places) the terms AI and ML are often used together. This can be considered
% The terms are often used interchangeably or together like \textit{AI and ML}, this can be confusing for someone not in this field.
\textbf{Første model for ANN (Artificial Neural Network) ble laget i 1943. ``Perceptron'' ble beskrevet i 1958, første multi-layer network publisert i 1965, ``continuous backpropagation'' ble utledet i 1960/1961, \ldots ANN/DL har altså en lang historie, i hovedsak drevet av forbedrede algoritmer, men det er først det siste tiåret at maskinvaren er blitt kraftig nok til at neuralnettverk, og da i første rekke ved hjelp av grafikkakseleratorer (GPU).}

Because this program used explicitly trained programs rather than developing its own "knowledge" from supplied examples, this falls into the category of ML and not the base category of AI. ML is distinct in that it attempts to deduce rules and go beyond human intuition using a complex net of interactions. 
\textbf{Turing's sjakkprogram er vel strengt tatt heller et eksempel på ``Rule-Based AI''? Alle reglene var bestemt på forhånd, men programmet prøvde for hvert trekk å finne det beste alternativet ut fra tilstanden på brettet og hvilke muligheter motspilleren ville få i sitt neste trekk.}
% (leads into architecture paragraph)

%Starting with explicitly trained programs. This falls in the category of AI and not ML.
%\textit{Learning, in the context of ML, describes the automatic search for a better representations.} 
%The network is presented with many examples and is trained, rather than explicitly programmed. 
Geoscientists may be more familiar with the concept of "calibration" when it comes to statistical models, which essentially is the same process. In the context of deep learning, ``deep'' refers to the number of layers contributing to a network, and thus the complexity of relationship between input variables. DL expands the ideas from ML using deeper networks, \textit{i.e,} more layers.
%\textbf{``e.g.'' betyr ``exempli gratia'', altså for eksempel; ``i.e.'' betyr ``id est'' -- ``det er'', ``altså''}
\textbf{Er ``backpropagation'' også en av forskjellene mellom ML og DL? Jeg er uklar om ``backpropagation'' brukes i ML utenfor DL.}

%Geoscientists may be more familiar with the concept of "calibration" when it comes to statistical models, which essentially is the same process. In the context of deep learning, deep references to the number of layers contributing to a network, and thus the complexity of relationship between input variables. DL expands the ideas from ML using deeper network, e.g. more layers. 
\input{Chapter3_Method/TiKZ/multilayer_mlp}
Figure \ref{fig:multilayer_mlp} illustrates a deeper version of the network displayed in Figure \ref{fig:one_layer_mlp}. A layer is a set of nodes. The connections between the layers are the trained units, also known as weights. To distinguish from deep learning, traditional machine learning is sometimes referred to as shallow learning. Linear regression (LR) is ML algorithm predating computers which is still useful today. Traditional LR can be derived using shallow learning methods. 

Intelligence, in the context of artificial intelligence, is still a topic of debate. Traditionally, a machine would be considered intelligent if it could beat a human at a given task. For computer chess, this was achieved in 1997 when IBM's DeepBlue beat Gary Karsparov. Researchers had learned how to build a chess-playing AI, but not a program that could generalize to anything beyond similar boardgames. \textbf{Raymond: DeepBlue var i hovedsak regel-basert, men Google's AlphaZero er (delvis) basert på DL.}

In retrospect, scientist have realized that this particular architecture is not be informative on human intelligence. See the paper from to get more information about the specifics \textbf{cite paper}. Based on psychology studies it is clear that the game of chess involves complex reasoning, search, perceptual and memorial processes. While one can solve chess using these abilities, one can also solve chess by taking radical shortcuts, that the human mind is not capable of. 
 

% Explain glossaries or words that are used a lot.
%Intelligence, in the context of artificial intelligence, is still a topic of debate. Traditionally, a machine would be considered intelligent if it would beat a human at a given task. For computer chess, this was achieved in 1997 when IBM's DeepBlue beat Gary Karsparov. Researchers had learned how to build a chess-playing AI, but not a program that could generalize to anything beyond similar boardgames. In retrospect, scientist have realized that this particular architecture is not be informative on human intelligence. See the paper from to get more information about the specifics \textbf{cite paper}. Based phsycology studies its clear that the game of chess involves complex reasoning, search, perceptual and memorial processes. While one can solve chess using these abilities, one can also solve chess by taking radical shortcuts, the human mind is not capable of. 
%Researchers became aware that they had learned less to nothing about how the human mind works. The original understanding of machine intelligence has been abandoned in search for a more complete definition. \textbf{chollet google artikkel}

There are several different types of machine learning, each suitable for solving different tasks. Figure \ref{fig:machine_learning_categories} shows the types of ML and their subcategories. These subcategories also exist for deep learning, the only difference being the number of layers used.
%, but in order to keep this as general they are described from the above level. Keep in mind that their deep learning cousins can be referred to by simply adding the prefix 'deep'. 
% Example on deep reinforcement learning.
%The frontier of chess playing programs is AlphaZero. The deep reinforcement learning architecture trained is using self-play. Without having any previous knowledge of rules. 

%Supervised learning is the part of machine learning concerned with learning the relation between input data, x and labelled data, y. Regression predict continuous values. Replicating a function. Classification is discrete, since it assigns a category to the input. 
%Reinforcement learning is a goal-oriented algorithms, most known for playing chess, solving labyrinths and lately \textcolor{red}{for?} active flow control \textbf{Cite Jean Rau, three papers}. \textcolor{red}{(Nytt avsnitt?)} 
%Unsupervised learning tries to detect patterns in unlabelled data. This includes clustering and dimensionality reduction. Dimensionality reduction has been used by climatologist for decades in order to remove seasonal variation \textbf{cite Benestad}. Unsupervised and reinforcement learning is out of the scope of this thesis and will not be discussed further.
\input{Chapter3_Method/TiKZ/graph.tex} 

\begin{itemize}
    \item \textbf{Supervised learning}: part of machine learning concerned with learning the relation between input data, x and labelled data, y.
    \begin{itemize}
        \item Regression\\predict continuous values. Replicating a function.
        \item Classification\\discrete, since it assigns a category to the input.
    \end{itemize}
    \item \textbf{Unsupervised learning}: Detecting patterns in unlabeled data.
    \begin{itemize}
        \item Clustering\\Grouping a set of data points into a predescribed number of groups
        \item Dimension reduction\\Reducing the number of random variables under consideration.
    \end{itemize}
    \item \textbf{Reinforcement learning}: Goal oriented algorithms.
\end{itemize}

\subsection{Autoregressive models} \label{sec:ARmodels}
The autoregressive model (AR) is a form of linear model where values from previous time steps are included as predictor variables. 

Description of variants of symbols used in equations. \textbf{More suitable as a table? or sentence?}
\begin{enumerate}
    \item y - one value y
    \item \textbf{y} - vector y
    \item \textbf{Y} - matrix y
    \item $\bar{\textbf{y}}$   - mean of y vector
    \item $\tilde{\textbf{y}}$ - true value of y vector 
    \item $\hat{\textbf{y}}$   - estimated y vector
\end{enumerate}

\begin{equation} \label{eq:AR_traditional}
    \hat{y}_n = \beta_0 + \sum_{i = 1}^{N} \tilde{y}_{n-i} \beta_{i}
\end{equation}
Equation \eqref{eq:AR_traditional} describes how to make a prediction, $\hat{Y}_n$ based on the optimal weights, $\beta_i$. $n$ denotes a particular time step, while $N$ denotes the order of the model, \textit{i.e,} the total number of time steps used to predict the next values. $\tilde{y}_{n-i}$ describes the true value of the predictor variable at time step, $t=n-i$, where $i$ is a counter going backward in time. The term $\beta_0$ is the bias (intercept). This corresponds to the intersection of a function on the y-axis.
\begin{equation} \label{eq:AR_expression}
    \hat{y}_n = \beta_0 + \sum_{j=1}^p x_j\hat{\beta}_j + \sum_{i = 1}^{N} y_{n-i}\hat{\beta}_{i}
\end{equation}
Expanding the traditional AR model to include other predictors, $X$ yields the expression in Equation \eqref{eq:AR_expression}. $p$ denotes the number of predictors. The other symbols are described above referring to Equation \eqref{eq:AR_traditional}.

Equation \eqref{eq:AR_solution} describes the optimal solution $\mathbf{\beta}$. Each predictor variable get a weight, $\beta_i$. Let $X^*$ be the concatenation of X and Y. In mathematical terms, $X^*=[X, Y]$. Using mean squared error loss, there exist an analytical solution to the optimization problem. 
\begin{equation} \label{eq:AR_solution}
    \mathbf{\beta}  = \left( \mathbf{X}^{*^T}\mathbf{X}^* \right)^{-1}\mathbf{X}^*\tilde{\mathbf{y}}
\end{equation}
The optimal solution is the best solution based on the training data available. The analytical solution is computationally very fast, as long as the matrix ${\mathbf{X}^*}^T\mathbf{X}^*$ is non-singular and thus its inverse exits.

\section{Artificial Neural Networks} \label{sec:artificial neural networks}
Artificial neural networks (ANN) are composed of artificial neurons and weights.  

Returning to Figure \ref{fig:one_layer_mlp} again, it illustrates nodes as circles and weights as arrows. It is an example of a 2-layer ANN. The nodes are structured in layers, illustrated using different colors. The input layer contains four input nodes, the hidden layer five nodes, and the output layer three nodes. The dimensions of the input and output layers are determined by the task at hand. The number of hidden layers and the number of nodes are tunable parameters, called hyperparameteres. Nodes of one layer are only connected to adjacent layers. Weights are the relative strength of the connections between nodes in neighbouring layers. %All networks have input, output, and zero or more hidden layers.
Large networks of these simple neurons are able to perform complex calculations. 
\input{Chapter3_Method/TiKZ/activation_mlp}

Figure \ref{fig:activation_one_node} shows the computation which takes place in a node in the hidden layer, a dot the middle column in Figure \ref{fig:one_layer_mlp}. The sum of the weighted input and bias are sent trough the activation function, $g$, producing the activation. The activation function is a hyperparameter, set before the training starts. Popular choices are rectified linear unit (ReLU), sigmoid or tanh-function. These are shown in Figure \ref{fig:activation_function_example}.
\begin{figure}
    \centering
    \includegraphics[scale = 0.4]{Chapter3_Method/figs/activation_functions_and_derivatives.png}
    \caption{Activation functions and their derivatives.}
    \label{fig:activation_function_example}
\end{figure}
Equation \eqref{eq:activation_hidden_pass} describes the activation of a node in a arbitrary layer, L. $b_L$ denotes the bias, $w_L$ is the weights matrix. and $n$ is the number of input nodes. $g_h$ denotes activation function in hidden layer and $g_o$ is the activation function chosen for the output layer. For regression problem, $g_o$ is linear like.
\begin{equation} \label{eq:activation_hidden_pass}
    \textbf{a}_L = g_h(\sum_{i=1}^n \textbf{W}_{L, i} \textbf{x}_i + b_L)
\end{equation}
\begin{equation} \label{eq:output_pass}
    \textbf{a}_{L+1} = g_o(\sum_{i=1}^n \textbf{W}_{L+1, i} \textbf{a}_{L+1} + b_{L+1})
\end{equation}
\textbf{Thought to self, Notation is trick when the next layer is the output. Since the activation-function usually is different in hidden layers and output layers. }
% Recursively moving things to next layer.
The activation's of the first layer is weighted, and passed to the output layer, according to Equation \eqref{eq:output_pass}.

\subsection{Convolutional neural networks} \label{sec:convolutional neural network}
%\textit{According to the philosophy underlying deep learning approach, if we have a reasonable end-to-end model and a sufficient data for training it, we are close to solving the problem}. (Shi et. al., 2015). 

Computer vision is a field of artificial intelligence concerned with interpreting the visual world. One popular structure for visual tasks is the convolutional neural network. %Its said to resemble the visual cortex, the centre in the brain which processes the visual information. 
\input{Chapter3_Method/TiKZ/2D_image_to_3D_tensor}
Computers see images as a grid of numbers, often decoded in red, green and blue (RGB) channels. Figure \ref{fig:2D_image} shows the transformation of a two-dimensional image to a 3-dimensional tensor. The ``P'' shows the connection between one pixel (``picture element'') and a volume. Each of the grid cells (pixels) contains the signal from the color decoded into values ranging from 0 to 255. The machine needs to learn how to extract the necessary information about these pixels to perform a task. More layers increase the model's ability to extract these complex structures, resulting in an improved model performance. 

\input{Chapter3_Method/TiKZ/convolution}
Figure \ref{fig:convolution} shows the mathematical operation convolution as the sum over element-wise multiplication of the filter and input. The filter is blue, this is placed over the filled green section, producing the red output pixel. The entire red grid is called a feature map (output map). The green grid is the input, overlaid with blue illustrated the pixels contributing to the activation, red pixel. In Figure \ref{fig:convolution} this would be the value 4. \textit{Receptive field} is known as the pixels contributing to the activation in a pixel (i.e. the value). For instance the receptive field of the shaded red pixel is the shaded green submatrix.

\input{Chapter3_Method/TiKZ/convolution_connection_between_layers}
Convolving a filter over the input image generates a feature map. If it happens to be the last layer, it is common to refer to the results as the output instead, even though there is no difference. Figure \ref{fig:convolution_padding} shows a 2D-convolution with filter of size $3\times 3$. Filters are often square (not a strict requirement) and the height, $f_h$ and $f_w$ are odd numbers (not a strict requirement either). The origin is the position of the kernel which is above the current output pixels. The connections between the layers are intended to illustrate the part contributing to the pixel, as well as highlighting the receptive field. In order to include the outermost pixels, the input area is padded with zeros around the edges (shown as gray in the figure). 
\input{Chapter3_Method/TiKZ/2_layer_convolutional}
It is worth noting that the same structures are given different names, based on their position in the network. The output is the feature map resulting from convolving the last layer. They are both activations, computed from the values and weights from the previous layer. The number of channels in the first layer and the numbers of feature maps in the subsequent layers are both simply stacks of grids containing values.

Working with RGB images requires 3D convolution; since the dimensions of the input determines the dimensions of the convolution, it is commonly referred to as simply convolution. As mentioned earlier, neural networks are strucured as a stack of layers. Each layer is again a stack of channels or feature maps. The output from the previous layer becomes the input to the next layer. Feature maps, activations, and outputs are all the result of a convolution, produced at different points within a neural net. The activations are computed based on an input volume, including information across channels. A 3D convolution collapses information on multiple colors into a single value.

Figure \ref{fig:conv_layers} shows a two layer convolutional neural network trained on RGB-images. The input layer is an RGB-image. The first convolutional layer has seven channels (feature maps), these are produced by seven filters. Filters are trained to extract useful features. The second convolutional layer is produced by five filters, all convolving layer 1. This is a simplified network, made shallow for illustrative purposes. Function CNNs require many layers to extract useful information from an image. %Networks are usually a lot deeper. 
Given raw input (\textit{i.e,} normalized images), the first layers detect low level features like edges, corners and circles. Later layers assemble the features to more complex structures like houses or dogs. The dashed volumes represent the receptive field for different pixels, illustrated as circles. Since each of the layers depend on the previous one, the receptive field of the a node, $P$ depends on a large portion of the input image. Small filters allow you to focus on small features in the data, while larger filters allow you to identify coarser relations.

Unlike fully connected layers (see Figure \ref{fig:one_layer_mlp}), the nodes in the output layer are not connected to all the input nodes, only the nodes within their receptive fields. The filters contain the trained units. Its dimensions determine the size of the feature it can detect. One convolution (using one filter) searches for a single feature over the entire image. When it finds this particular feature it activates, propoagating this signal into the feature maps.% Reduces the number of trainable parameters and making it more robust against overfitting.

\subsection{Recurrent Nets} \label{sec:reccurent_nets}
% Recurrent betyr tilbakevendende
% Sequential modelling has had a great success in applications such as machine translation and speech recognition 
\input{Chapter3_Method/TiKZ/rnn}
A recurrent neural network (RNN) is a class of artificial neural networks developed for studying patterns in sequential data such as time series, audio, or text. Figure \ref{fig:rnn} shows the structure of a simple RNN. In very simplified terms, a RNN receives an input, produces an output and passes hidden state back to itself. In Figure \ref{fig:rnn}, A denotes a recurrent unit, $x_t$ denotes the input data and  $h_t$ denotes the hidden state. The output from an arbitrary node, $t$ is $y_t$. %The loop explains the origin of the name recurrent neural network. 
The hidden state contains the information about what you have learned so far. The output at each time step is dependent on the previous inputs. 

\input{Chapter3_Method/TiKZ/rnn_unrolled}
Figure \ref{fig:rnn_unrolled} shows the recurrent network unrolled in time. This way of structuring it resembles the earlier structures like ANN (see figure \ref{fig:one_layer_mlp}). The connection between the nodes %in this kind of networks 
are a directed graph along a temporal sequence. The hidden state from the previous step is fed into the next. $h_0$ is only dependant on $x_0$, while $h_t$ is dependent on $x_0, x_1, \cdots, x_t $. This example shows a one layer recurrent network. All time steps are passed through the same node. The RNN reuses the weights on the input and hidden states for all time steps. Let t denotes the length of the training sequence. The "memory" stores the useful information from $x_0, x_1, \cdots, x_t $ needed to make a prediction, performing the same task on all inputs along the sequence. This reduces the complexity of parameters and in turn lowers the risk of overfitting, obtaining a more general relation between input and output.
% It is used to predict the next word in a sentence or the next tone in a song.  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Learning long term dependencies can be a challenging. Learning is done by backpropagating the error signal thought the network. Working with longer sequences, the error signal tends to approach zero or infinity. Exploding gradients can cause the weights to oscillate. Learning from small or vanishing gradients takes ages, or might not learn anything at all. 
%More advanced forms of recurrent nets control the information flow using gates. \textbf{cite 1997 and cite 1999 learning to forget.}

\subsubsection{Long Short-term memory network, LSTM} \label{sec:lstm}
%\textbf{Use recurrent self-connections instead of loops} \textcolor{red}{Ufullstendig setning}.
In order to address challenges in predicting long sequences presented in the previous section, Sepp Hochreiter and Jürgen Schmidhuber created the LSTM network. Their design, documented in a 1997 paper (\textbf{citation)}, outperformed previous memory networks by regulating the flow of information provided to a recurrent network at each time step.
%The memory unit introduced by Sepp Hochreiter and Jürgen Schmidhuber in 1997 set performance records in multiple domains. The paper introduces gates to regulate the flow of information. 
A gate is structure that can be opened or closed. Having values in the range zero to one, it truncates the noise signal from the input and the output. They propose a new method for learning %an approach for constant error flow 
in order to alleviate the issues with exploding or vanishing gradients. The original memory cell contains input and output gates. Gers et. al. 1999 proposed to add an additional gate, the forget gate. The idea was to enable the LSTM to reset parts its own memory. Resetting these parts releases internal resources and enables you to learn even more. 

Because of the interdisciplinary nature of this thesis, this section provides a thorough walk through the LSTM cell and its relevant equations. The memory unit is displayed in Figure \ref{fig:lstm_unit}. The legend describing the different components of the computational graph is available in Figure \ref{fig:legend_lstm}. The information flow thought the cell is regulated by three gates; the forget gate, the input gate and the output gate. These gates are neural networks. The forget gate learns which part of the cell state (the long-term memory) it should forget. The input gate learns the information from the input it should add to the cell state. The output gate learns which information it should send to the output. 

The process of training is repeated until the network reaches an acceptable performance. In other words \textit{the extent to which this potential can be exploited is limited to the effectiveness of the training procedure applied}. There is two types of techniques involved in the training procedure, forward- and backward pass. \textit{For each training instance, the algorithm feeds it to the network and computes the output of every neuron in each consecutive layer \textbf{fix here} (this is the forward pass, just as when making a prediction).} 
% A forward pass propagates the input trough the network. In the same way you would make a prediction. 
The error signal is the difference between the true and the predicted values. The learning algorithm involves backpropagating the error signal in order to compute a suitable adjustment of the weights. For this particular architecture, LSTM, the gates are trained neural network. Learning which information to let thought the gate.
% More details concerning gradient based learning in Section \ref{sec:backprop_learning_algorithm}.
% Introducing some terminology, a prediction is the result of forward passing the information through the network. 

\subsubsection{Feed forward} \label{sec:forward_pass_lstm}
\input{Chapter3_Method/TiKZ/subplot}
\input{Chapter3_Method/TiKZ/subplot2}
In combination Figures \ref{fig:LSTM_all} and \ref{fig:LSTM2} shows a computational graph of the LSTM unit. The relevant equations are shown in the figure text, making it easier follow the computations. The following subplots highlights the graphs relevant for gates and necessary calculations. 
%highlighting key computations along with their respective equations. 
Figure \ref{fig:LSTM_all} show subplots of the components, while Figure \ref{fig:LSTM2} show the memory flow and the assembled memory unit.

% The cell state plus gates.. 
% (1) if you start with this you should update the subplots.
Figure \ref{fig:cell_state_information_flow} shows the information flow of the cell state. The LSTM has a long term memory, known as the cell state and the short term memory referred to as the hidden state. The cell state is affected by some linear interactions, this is very simple, thus the flow often remain unchanged. The hidden state is the output passed to the next cell. Structures like gates regulate the flow of information. The gates are neural networks layers with a particular activation function. It is called sigmoid function, named after the greek letter sigma, shaped like an S. Truncating the output from the gates to range between zero and one. Figure \ref{fig:activation_function_example} shows the sigmoid and tanh functions. Zero represents a closed gate, while one describes a wide-open gate. At each time step in the sequence the LSTM receives input $x_t$ and the previous hidden state, $h_{t-1}$. These are passed trough all gates. 

% The forget gate
In order to make a prediction requires the forward propagation of information through the network. Figure \ref{fig:forget_gate} shows that based on the new input and the previous hidden state, the forget gate determines which instances from the memory to remove. Regulating the information that stays in memory frees up space, allow you to learn new things. The gate is initialized to 1, thus it can't forget anything until it has learned to forget. Exhibiting the same behavior as the original LSTM units. 

% The input gate 
Figure \ref{fig:input_gate} shows two processes, one determining the candidate information based on the input and two the computations of the input gate. The candidate information is filtered by the multiplicative input gate. This determines what information to add to the cell state.

Figure \ref{fig:update_memory} shows how the cell state is updated. Using mutiplicative gates, first the forget gate removes information. Then the output from input gate adds the useful information from the input to memory. The input gate regulates what information to store from the input. The aim of the input gate is to clean the input by reducing the noise signal. These computations are also shown in Figure \ref{fig:input_gate}.

Figure \ref{fig:output_gate} illustrates how the output gate gets updated. Passing the cell state thought a tanh-function, truncating it to take values between minus one and one. The output gate determines what to remove from the truncted cell state and pass as the hidden state. This gate aims to remove noise from the output. Preventing misrepresentations of the hidden state (short-term memory). This forces the hidden state to always take values in the range minusa one to one. \textbf{cite LSTM 1997} In summary, at each time step some memories are removed and others are added.  
% \textbf{cite colah blogpost ?}
% http://colah.github.io/posts/2015-08-Understanding-LSTMs/

\subsection{Convolutional LSTM}  \label{sec:convolutional_lstm}
A variant of the LSTM network is the convolutional LSTM (ConvLSTM). First developed for precipitation nowcasting in 2015. The only difference between this and the general LSTM is that the standard fully connected neural networks (see figure \ref{fig:one_layer_mlp}) is replaced by convolutional neural network (see figure \ref{fig:convolution_padding}). This allows the LSTM network to support multi-dimensional data, capturing the spatiotemporal structure in the data. This architecture is has the potential to solve problems in time and space. 

\input{Chapter3_Method/TiKZ/inside_conv_lstm}

Keeping the same structure as in Figure \ref{fig:lstm_unit}, while making small changes to the equations used to forward propagate the input. The multiplicative gates are now replaced with convolution. 
Figure \ref{fig:inside_conv_lstm} show the dimensions of states and input in a ConvLSTM. Equations \eqref{eq:CLSTM2_forget_gate} to \eqref{eq:CLSTM5_hidden_state} is describes the forward propagation through a covolutional LSTM. Here $\circ$ denoted the Hademand product, which is a component wise multiplication, and * is convolution. Let t denote the time step, $\sigma$ is the activation function, $tanh$ be tanges hyberbolicus, $W_{\text{component}, \text{gate}}$ denote the trained weights of components and gates, $H_{t}$ denotes the hidden state at time t, $C_{t}$ is the cell state at time t. 

\begin{equation} \label{eq:CLSTM2_forget_gate}
        f_t = \sigma \left( W_{xf}*x_t + W_{hf}*H_{t-1} + W_{cf}\circ C_{t-1}+b_f \right)
\end{equation}

\begin{equation} \label{eq:CLSTM1_input_gate}
    i_t = \sigma \left( W_{xi}*x_t + W_{hi}*H_{t-1} + W_{ci}\circ C_{t-1}+b_i \right) 
\end{equation}

\begin{equation} \label{eq:CLSTM3_cellstate}
        C_t = f_t \circ C_{t-1} +i_t\circ tanh\left( W_{xc}*X_t + W_{hc}*H_{t-1} + b_c \right)
\end{equation}

\begin{equation} \label{eq:CLSTM4_output_gate}
        o_t = \sigma \left( W_{xo}*X_t + W_{ho}*H_{t-1} + W_{co}\circ C_{t}+b_o \right)
\end{equation}

\begin{equation} \label{eq:CLSTM5_hidden_state}
        H_t = o_t \circ tanh \left( C_t \right)
\end{equation}

\subsection{Padding (and Stride?)} \label{sec:padding}
% Add equation to calculate how much zero padding is needed to make a prediction of the same size.
The convolution operation shrinks the dimensions of the feature map, according to Equation \eqref{eq:output_size}. The degree of shrinking  depends on the  filter size, padding and stride. Padding zeros along the edges has an additional benefit of including the signal from the boundaries. Stride determines how far you move the filter between convolutions. In other words it control how you convolve around the input volume. Figure \ref{fig:convolution_padding} shows a example with a zero padding of one. This is sufficient to keep the input dimension using a filter of dimensions $3 \time 3$. Which can be shown by plugging the dimensions into \eqref{eq:padding_same}.

For a arbitrary image. Some definitions,
\begin{enumerate}
    \item image size, $n\times n$
    \item filter size, $f\times f$
    \item padding size, p
    \item stride, s
    \item output size, $o \times o$
\end{enumerate}

\begin{equation} \label{eq:output_size}
    o = \frac{n+2p-f}{s} + 1
\end{equation}
In some cases it can be useful to have the same shape of input and output. This is called \textit{padding same}. Solving the equation \eqref{eq:output_size} for $p$ using $o=n$ yields the following expression.
\begin{equation} \label{eq:padding_same}
    p = \frac{n\left(s-1\right)-s+f}{2}
\end{equation}

\subsection{Learning algorithm} \label{sec:backprop_learning_algorithm}
Learning is a time consuming task. The fundamental trick in deep learning is to use the performance metric as a feedback signal to adjust the weights. It adjusts in the direction of the lowest loss score for the current example (i.e. the current batch). The adjustment is the job of the optimizer, which implements backpropagation algorithmn which is the central learning algoritmn. This section aims to build a understanding of backpropagation trough time without referring to any significant mathematics. If you are interested in the mathematics behind this, read X and Y. 

\textit{Learning means finding a suitable representation of model parameters that minimize a loss function for a given set of training data samples and their corresponding targets.}

\section{No need to correct beyond this point, the following text is just notes on things i might write in the future. Some figures might have snuck passed this soft barrier thats all.}


\subsubsection{Transforming data} \label{sec:transforming_cloud_cover}
\textbf{Inverse of sigmoid is often refered to as logistic sigmoid?}
Transformation of data us useful for avoiding predicting unphysical values. As well as keeping digits from exploding.
A common approach for values in the range zero to one is using the inverse of the sigmoid function, shown in figure \ref{fig:activation_func_plus}. The transform uses values from the entire real axis $(-\infty, \infty)$. 
%\input{Chapter2_Theory/tikz/sigmoid.tikz}
Tranforming the data using sigmoid and then squazing it back between 0 and 1. 
Continous models can learn out-of-sample values. In this case it would be unphysical.

\subsection{Loss/ Metrics}  \label{sec:metrics}
In order to acquire a certain skill you need a measure determining how close you are. 
Use the sum of square or absolute values in order to not penalize point on the lower side of the line. Or not having to deal with negative distances. 
\begin{equation} \label{eq:mse}
    MSE(\hat{y},\hat{\tilde{y}}) = \frac{1}{n} \sum_{i=0}^{n-1}(y_i-\tilde{y}_i)^2
\end{equation} 

\begin{equation} \label{eq:ase}
    ASE(\hat{y},\hat{\tilde{y}}) =  \sum_{i=0}^{n-1}(y_i-\tilde{y}_i)^2
\end{equation} 

\begin{equation} \label{eq:r2}
    R^2(\hat{y}, \tilde{\hat{y}}) = 1 - \frac{\sum_{i=0}^{n - 1} (y_i - \tilde{y}_i)^2}{\sum_{i=0}^{n - 1} (y_i - \bar{y})^2}
\end{equation} 
where mean value of $\hat{y}$ is defined as $\bar{y} =  \frac{1}{n} \sum_{i=0}^{n - 1} y_i$. $R^2$ describes how much of the variation in the dataset you are able to capture with your model.

\subsection{Generalization} \label{sec:generalization}
% Move overfitting here
Finding a suitable curve for a set of points. Working with real data, noise is inevitable. In order to compensate, data is split into training and test (validation) sets. % better to call it something like generalization..?
Overall goal is to achieve the most general relation. \textit{For prediction purposes they can sometimes outperform fancier non-linear models, especially in situations with small numbers of training cases, low signal-to-noise ratio or sparse data.} \textbf{Hastie et al.} Overfitting becomes evident when you have a increase in the difference between the test- and training error. In non-mathematical terms, you have adjusted to much to the training data and where not able to find the general relation, program or "rules". See Figure \ref{fig:linreg_overfitting} 

\begin{figure}[hp]
    \centering
    \includegraphics[scale = 0.5]{Chapter3_Method/figs/generalization.png}
    \caption{Fitting at different levels. The optimal fit is the most general one. This is applicable to many cases. For the traditional autoregressive models, the predictor variable is the true value in the previous time step. \textbf{I'll make my own figure if we decide it should be a part of my thesis.}}
    \label{fig:linreg_overfitting}
\end{figure}

Its relevant for all ML algorithms but easiest to visualize for linear regression.
\textit{Overfitting a model is a condition where a statistical model begins to describe the random error in the data rather than the relationships between variables.}

\subsection{Automatic Optimization} \label{sec:hyperparam_tuning}
Keras-tuner. Tuning hyper-parameters.
\textit{A hyperparameter is a constant parameter whose value is set before the learning process begins.}

\textbf{Explain all the params you tune. Might be beneficially with a figure. See Rune's MS-thesis.}

\textit{Learning to forget found the best accuracy when using learning rate decay.}


Although theoretically fascinating it remains to see if LSTM provide a clear practical advantage over the autoregressive models.


