\chapter{Discussion - MOve content to ChX results and conclusion..}
conclusions we draw from this idealized example will apply equally well to
\section{Notes}
\begin{enumerate}
    \item Ar - modeller er uavhengige modeller, hvor naboer er trent på korrelert data, uten informasjon er
    \item Nabolag rundt transformasjonen. 
    \item The distribution of values
    by feature is shown in fig. 2.
    \item Create correlation matrix using 
    \item We will keep in mind both the scores of 0.743 and 0.782, serving
    as fair baselines for a very simple, and slightly more sophisticated
    regression fit, respectively.
    \item Plotting R2 vs epoch, set ylim to 0,1. Not interresting to see where it learns.
    \item Summaries the best five architecture 
    \item A drawback of introducing more layers
    is that it increases the complexity, and thereby the chance of
    \item emphesizing a good agreement for latitudinal variation.
\end{enumerate}

\section{Good phases}
\begin{itemize}
    \item  In its most simple form, the diffusion equation is given by
    \item By using a finer grid one can usually get better approximations
    \item The comparison will focus on computational time and accuracy.
    \item This was chosen after
studying the development of the loss function as a function of number of iterations
    \item and is mandatory in
more advanced architectures(e.g. residual nets) where a constant spatial
dimension is demanded.
    \item a list of examples and so forth.
    \item (I will reference to source code/project part where relevant!)
    \item out of sample precision.
    \item Learn how to create plots with a zoomed in view.
    \item sufficiently large
    \item Viktig poeng. \textit{However, both academic
researchers and practitioners alike acknowledge the
need to make tests on the actual data set that is
subject of interest, as well as dedicating time and
resources to tune hyper parameters}
    \item methods for tabular data vs images
    \item I have opted to use
    \item We have also modified our own code for a dense feed forward neural
network produced for Project 2, see
    \item Gradient methods are at the heart of every machine learning algorithms. 
\end{itemize}


\textit{This chapter summarizes the results presented in previous chapter and discusses their implications. It starts with a discussion about the model setup and the performance of the model for different parameters and setup. Afterwards, the input data are discussed before model performance is considered. Lastly, ... uncertainties in parameter choice/ dataset..? Is it possible to get a uncetaity from the ERA5 data and satellite data..?} 


% above is copied
Historically cloud specific parameterization have been developed. The categorisation into cloud regimes have lead to problems. There is a urgent need for general purpose parametrization of clouds. Its believed that focusing on fewer variables stands a better chance of reducing the uncertainty associated with modelling cloud processes. Employing a hierarchical modelling approach, focusing on macrophysical properties.

Possible extension: Another possibility as \cite{Fowler1996LiquidAssumptions} mentions, is to parameterizing microphysical properties based on the \acrshort{cfc}.

\section{Artefact}
To årsaker til at man sannsynligvis får mer med skyer der det er aersoler,  ... finn i gullboka
\begin{enumerate}
    \item en
    \item to
\end{enumerate}


\section{Other}
Its worth noting that the satellite data using in this thesis is not included in  \citeauthor{Stubenrauch2013AssessmentPanel} study, but its illustrates nicely the large differences among other datasets. 


Siden det er en passive sendfor kan man forvente at cloud masks blir overestiment jo mer off nadir man beveger seg pga. strålen går lengder det det blir et større område å treffe sky eller aerosol på. \citeauthor{Maddux2010ViewingProducts}


It can be worth noting that the all sky radiance's from \acrfull{msg} in the period 2003-2012 is included in the assimilation. This is the same satellite that provides the cloud mask. 
\textbf{One sentence explaining that we have judged the appropriate use and have come to the conclusion that using the assimilated variables and not parameterised are appropriate for this application.}


\textbf{A reasonable accurate parameterization should provide a good estimate of the target variable, here cloud cover in isolation. Also on a higher level contribute to a improved model performance in the term of uncertainties attributed to a variable or process.}  

It is not impossible for models to perform equally well. Evaluate how well a model remake a phenomna. Hugo : det er medre optimsere for å treffe et spesilt fenomen of det er veldig viktig. 

%It is possible to achieve the same score with several  models. For this particular case it could be an idea to test the response of the model to particular weather phenomenas, that reflect a lot of sunight and would be necessary to hit to get a good over all performance. \textbf{Få et eksempel fra trude} 

It is important to know if the model is performing good for the right reasons.

Based on the small set of experiments run in this section, there is 
Datasets publish have different versions. Propose a suggestion for the content on \acrshort{ecc} v2.

To order to perform more complex experiments than shown in this section, more work need to be done behind the scene's to ensure better flow of data into memory. The combination of batch size and sequence length determine the load on memory per weight update date. The are inverse of each other, for short sequences the batch sizes can be larger. This is not the same as stating that this will be the optimal solution. Who knows the optimal solution might be outside the search space available on the used environment. Advances in hardware, algorithms and X drive advances in machine learning, as mentioned in Section \ref{ch:num_methods} \textbf{add more specific section}. The optimal solution may have always been out of reach. 

Unlike the \cite{}



\begin{enumerate}
    \item Argumenter for at andre ikke har trent denne modellen på så store dataset. 
    \item Skyer har en temporal opplæsning på en time så det er uaktelt å endre på denne for å trene modellen. 
\end{enumerate}


The statistical properties are slightly different for the different test sets.  

Short sequence length to reduce the computational complexity of experiments. 

No efforts have been made to assess the impact of the artefact on the performance.

One variable was not found to increase the performance.


data sets can be reduced without significant loss of information

A drawback of this study was not to account for correlations between radiomics features and the ROI, but such corrections were
not performed in this .

Land ocean mask study statistics of subgroups.

Batch size describes the number of samples used to make a weight update. The number of epochs determines how many times the datasets sees all samples. The learning rate determines the magnitude of the step taken in the direction of the steepest gradient, as described in Section X.

\textit{Learning means finding a suitable representation of model parameters that minimize a loss function for a given set of training data samples and their corresponding targets.} 

Learning is a time consuming task. The fundamental trick in deep learning is to use the performance metric as a feedback signal to adjust the weights. It adjusts in the direction of the lowest loss score for the current example (i.e. the current batch). The adjustment is the job of the optimizer.


kan være verd et forsøk å ha batch size på en for å få sjekket ut higher level abstractions.

Two clusteres are used in this experiment Wessel on UiO and ex3 from simula for the machine learning tasks. See Table
\ref{tab:hardware}
\begin{table}[ht]
    \centering
    \begin{tabular}{c|c}
        Device &  Type  \\
        RAM  & 
        CPU (32 stk)  &
    \end{tabular}
    \caption{Hardware specifications for our testing environment on Wessel. Operativ system er Red Hat Enterprise Linux 7, RHEL7.}
    \label{tab:hardware_wessel}
\end{table}


%are used for \acrshort{convlstm} using buil in fuctionality in tensorflow. This is not available for the ar-model.

%\textbf{Without varying batch sizes you lose additional samples, this might not be necessary but current version run with drop remainder batch}

\textbf{for reaching to performance characteristics similar to }
\begin{enumerate}
    \item Ser at X**T dot X blir stor uten transformasjon derfor brukes alltid det. Sjekker om det hjelper med transformasjon.
    %\item The tf.keras API simplifies many aspects of creating and executing machine learning models. 
    \item distribution strategies
    \item Note that the numbers here are for demonstration purposes only and may not sufficiently produce a model with good quality.
    \item You have full control over how you want your data to be distributed across workers and devices, and you must provide an input\_fn to specify how to distribute your data.
    \item Bandwidth on the other hand is the amount of data the web hosting server will allow your website to send and receive over the internet in a particular time period. Both disk space and the bandwidth or data transfer are measurable and are usually measured in gigabytes now-a-days.
    \item Det er ikke en gang sikkert du vaf klar over den suboptimale løsniingen før du prøvde med mer data. 
    \item (GPU) In certain applications requiring massive vector operations, this can yield several orders of magnitude higher performance than a conventional CPU. 
    \item Distributed computing also refers to the use of distributed systems to solve computational problems. In distributed computing, a problem is divided into many tasks, each of which is solved by one or more computers,[4] which communicate with each other via message passing.[5] 
    \item Nevertheless, it is possible to roughly classify concurrent systems as "parallel" or "distributed" using the following criteria: In parallel computing, all processors may have access to a shared memory to exchange information between processors.[18] --distributed programming have its own memory, originally this was used for running programs on computers at different geographical locations. 
    \item Complex computations cause memory growth.
    \item There is no need to convert existing code to use TFRecords, unless you are using tf.data and reading data is still the bottleneck to training. See Data Input Pipeline Performance for dataset performance tips.
    \item Now, let’s start building up the iterators. Tensorflow has provided four types of iterators and each of them has a specific purpose and use-case behind it.
    \item This is usually the skeleton code of how a Dataset and iterator looks like.
    \item It may involve empty function declarations, or functions that return a correct result only for a simple test case where the expected response of the code is known.
    \item Reading the first byte of a file from remote storage can take orders of magnitude longer than from local storage.
    
    \item To choose the correct parameteres for you data pipeline and parallell programming you need to have an understanding of the environment you work on.
    \item I will start with deploy. Deploy should mean take all of my artifacts and either copy them to a server, or execute them on a server. It should truly be a simple process.
    \item Build means, process all of my code/artifacts and prepare them for deployment. Meaning compile, generate code, package, etc.
    \item A versatile code should support different types of archiitectures, hardware and api's.
    \item Making your code distribution aware. Having access to more than one GPU you can, create a stategy scope using tf, and with small changes making you model distribution aware.
    \item Two steps, data processing (on cpu) og model computation (on a gpu).
    \item latency = ventetid
    \item $@tf.function$ increases the speed of computations (I think this makes it a C code).
    \item Optimizing the tf.Graph also reduces the device peak memory usage and improves hardware utilization by optimizing the mapping of graph nodes to compute resources. 
    \item threads are ordered streams of instructions.
    \item concurrent/simultaneously == samtidig
    \item Making applications thread safe is diffucult, it essentially means having mutexes on you resources so that no two threads can access the same place in memory/ same process resources at the same time.
\end{enumerate}



Liste Hugo:
1. En AR model tar ca. et hundre delsekund etter jeg har reorganisert filene mine. Til å inneholde all dataen for en pixel, så ser ut som vi kan friskmelde den .. Fikke kjørt 161/4 pixler i går. Bare for sikkerhetskyld så har jeg lagt det opp sånn at når jeg først har lastet inn dataen tester den fra lag 0 til lag max. Da kan jeg lett sammenligne de første pixlene som kjører ferdig å gjøre meg noen tanker rundt dette. Testet det uten transformasjon og sigmoid -- det blir prosjektet for helga. 

** Siri hun hadde prediktere til et tidspunkt, informasjon 24 timer tilbake i tid.
Får man noe mer informasjonp for tidspunktet 24 timspunkt.

Plot korrelasjon mellom skyer og 24 t 
Autokorrelasjon, Test det for pixels. 

AR modeller som predikerer 6 tidspunkt frem. Da trenger du 

6 modeller, en for å tidspunkt 1 og 2, osv opp til 6.


Objective (når man beveger seg inn i ukjent terreng), 
1. Main objective høy kvalitets dataset for å kunne gjøre klimaprojeksjoner. 
2. Modellere AR og Evalurer
3. Modellere en LSTM og Evaluere


En måte å se på det på, hvor god er kombinasjonen av dataset og modeller. Er feilen så liten at det har en nytte effekt eller er feilen så stor at det ikke har noe praktisk nytte verdi. Da ligger nok feilen i datasettet. Det er ikke så mye å hente på modell siden om jeg har gjort det slik jeg burde.

Det ville overaske hugo om noen modeller som predikerer dette bedre basert på samme dataset. Det er ikke sikker det finnes data som er tilstrekkelig høy kvalitet grunnlags data datasettet kan bygges på.

Etter min mening er dette så bra dataset man får etter min mening, men idealt sett skule kanskje skyene også være integrert i noe reanalyse. 

For de dataene som finner idag får vi det, kan nok få bedre resultater av mer avanserte preprosseser.

Dataset future work
1. artefact -
2. hjørner -
3. nans -
  * 
  
I dataset. 
OBS pass på alternative dataset og styrker og svakheter på disse og hvorfor du gjorde det på denne måten. 
Tabell hvor du oppsummerer dem.

Skriv modellene i tekst og i tabell. 