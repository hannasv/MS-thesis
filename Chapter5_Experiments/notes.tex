
Two clusteres are used in this experiment Wessel on UiO and ex3 from simula for the machine learning tasks. See Table
\ref{tab:hardware}
\begin{table}[ht]
    \centering
    \begin{tabular}{c|c}
        Device &  Type  \\
        RAM  & 
        CPU (32 stk)  &
    \end{tabular}
    \caption{Hardware specifications for our testing environment on Wessel. Operativ system er Red Hat Enterprise Linux 7, RHEL7.}
    \label{tab:hardware_wessel}
\end{table}


%are used for \acrshort{convlstm} using buil in fuctionality in tensorflow. This is not available for the ar-model.

%\textbf{Without varying batch sizes you lose additional samples, this might not be necessary but current version run with drop remainder batch}

\textbf{for reaching to performance characteristics similar to }
\begin{enumerate}
    \item Ser at X**T dot X blir stor uten transformasjon derfor brukes alltid det. Sjekker om det hjelper med transformasjon.
    %\item The tf.keras API simplifies many aspects of creating and executing machine learning models. 
    \item distribution strategies
    \item Note that the numbers here are for demonstration purposes only and may not sufficiently produce a model with good quality.
    \item You have full control over how you want your data to be distributed across workers and devices, and you must provide an input\_fn to specify how to distribute your data.
    \item Bandwidth on the other hand is the amount of data the web hosting server will allow your website to send and receive over the internet in a particular time period. Both disk space and the bandwidth or data transfer are measurable and are usually measured in gigabytes now-a-days.
    \item Det er ikke en gang sikkert du vaf klar over den suboptimale løsniingen før du prøvde med mer data. 
    \item (GPU) In certain applications requiring massive vector operations, this can yield several orders of magnitude higher performance than a conventional CPU. 
    \item Distributed computing also refers to the use of distributed systems to solve computational problems. In distributed computing, a problem is divided into many tasks, each of which is solved by one or more computers,[4] which communicate with each other via message passing.[5] 
    \item Nevertheless, it is possible to roughly classify concurrent systems as "parallel" or "distributed" using the following criteria: In parallel computing, all processors may have access to a shared memory to exchange information between processors.[18] --distributed programming have its own memory, originally this was used for running programs on computers at different geographical locations. 
    \item Complex computations cause memory growth.
    \item There is no need to convert existing code to use TFRecords, unless you are using tf.data and reading data is still the bottleneck to training. See Data Input Pipeline Performance for dataset performance tips.
    \item Now, let’s start building up the iterators. Tensorflow has provided four types of iterators and each of them has a specific purpose and use-case behind it.
    \item This is usually the skeleton code of how a Dataset and iterator looks like.
    \item It may involve empty function declarations, or functions that return a correct result only for a simple test case where the expected response of the code is known.
    \item Reading the first byte of a file from remote storage can take orders of magnitude longer than from local storage.
    
    \item To choose the correct parameteres for you data pipeline and parallell programming you need to have an understanding of the environment you work on.
    \item I will start with deploy. Deploy should mean take all of my artifacts and either copy them to a server, or execute them on a server. It should truly be a simple process.
    \item Build means, process all of my code/artifacts and prepare them for deployment. Meaning compile, generate code, package, etc.
    \item A versatile code should support different types of archiitectures, hardware and api's.
    \item Making your code distribution aware. Having access to more than one GPU you can, create a stategy scope using tf, and with small changes making you model distribution aware.
    \item Two steps, data processing (on cpu) og model computation (on a gpu).
    \item latency = ventetid
    \item $@tf.function$ increases the speed of computations (I think this makes it a C code).
    \item Optimizing the tf.Graph also reduces the device peak memory usage and improves hardware utilization by optimizing the mapping of graph nodes to compute resources. 
    \item threads are ordered streams of instructions.
    \item concurrent/simultaneously == samtidig
    \item Making applications thread safe is diffucult, it essentially means having mutexes on you resources so that no two threads can access the same place in memory/ same process resources at the same time.
\end{enumerate}